{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf2a9d1c",
   "metadata": {},
   "source": [
    "## Important Librabies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98113f2e",
   "metadata": {},
   "source": [
    "## LinearRegression, LogisticRegression, Lasso, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eddc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model\n",
    "# LinearRegression,LogisticRegression,Lasso, Ridge\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.linear_model import Lasso, Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b349e5",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5299faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6844bfbf",
   "metadata": {},
   "source": [
    "## Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34473c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree  \n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f3e8e5",
   "metadata": {},
   "source": [
    "## XGBoost Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba29005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Extreme Gradient Boosting\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d561885",
   "metadata": {},
   "source": [
    "## Random Forest,Adaboost,Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c6d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# Adaboost\n",
    "# Gradient Boosting\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor,AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7672bfe",
   "metadata": {},
   "source": [
    "## KNN Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090c3c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Neighbour\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3571615",
   "metadata": {},
   "source": [
    "## K Mean Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f16c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Mean Clustering\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac0bf1",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12539507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb49e50",
   "metadata": {},
   "source": [
    "## Principle Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b643af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principle Component Analysis\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb03a4c1",
   "metadata": {},
   "source": [
    "## StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144fd4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4715fa8",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb8b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f0bbdf",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6501abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e70180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing scikit-learn and some commonly used machine learning algorithms\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor,AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.metrics import classification_report,confusion_matrix,recall_score,precision_score,f1_score\n",
    "\n",
    "# You can also import other utility functions and classes from scikit-learn as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c0dc3",
   "metadata": {},
   "source": [
    "## One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e16506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset with categorical data\n",
    "data = pd.DataFrame({\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston'],\n",
    "    'Weather': ['Sunny', 'Rainy', 'Cloudy', 'Sunny']\n",
    "})\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit the encoder on the categorical data\n",
    "encoder.fit(data[['City', 'Weather']])  # Specify the columns to encode\n",
    "\n",
    "# Transform the categorical data into one-hot encoded format\n",
    "encoded_data = encoder.transform(data[['City', 'Weather']])\n",
    "#print(encoded_data)\n",
    "\n",
    "# Convert the sparse matrix to a dense array\n",
    "encoded_data_array = encoded_data.toarray()\n",
    "#print(encoded_data_array)\n",
    "\n",
    "# Create a DataFrame with the one-hot encoded data\n",
    "encoded_df = pd.DataFrame(encoded_data_array, columns=encoder.get_feature_names_out(['City', 'Weather']))\n",
    "\n",
    "# Display the one-hot encoded DataFrame\n",
    "print(encoded_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66757e08",
   "metadata": {},
   "source": [
    "## Get Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c3496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a Pandas DataFrame with your categorical data\n",
    "data = pd.DataFrame({\n",
    "    'Color': ['Red', 'Green', 'Blue', 'Red', 'Green'],\n",
    "    'Size': ['Small', 'Medium', 'Large', 'Medium', 'Small']\n",
    "})\n",
    "\n",
    "# Use pd.get_dummies() to perform one-hot encoding\n",
    "encoded_data = pd.get_dummies(data, columns=['Color', 'Size'],dtype=int)\n",
    "\n",
    "# The resulting DataFrame contains one-hot encoded columns\n",
    "print(encoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7beef1",
   "metadata": {},
   "source": [
    "## Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07c7cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample dataset with categorical data\n",
    "data = [\"cat\", \"dog\", \"fish\", \"dog\", \"cat\"]\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "encoded_data = encoder.fit_transform(data)\n",
    "\n",
    "# Print the original data and the encoded data\n",
    "print(\"Original Data:\", data)\n",
    "print(\"Encoded Data:\", encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ecb8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Number of Columns: One-Hot Encoding generates several binary columns, according to the number of distinct categories, \n",
    "whereas Label Encoding only generates a single column for the categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f806d26",
   "metadata": {},
   "source": [
    "## Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95ae1664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "Original Text: This is a sample sentence with some stop words.\n",
      "Text with Stop Words Removed: sample sentence stop words .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sachin\n",
      "[nltk_data]     Wandre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text data\n",
    "text = \"This is a sample sentence with some stop words.\"\n",
    "\n",
    "# Tokenize the text (split it into words)\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Initialize the NLTK stopwords set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(len(stop_words))\n",
    "\n",
    "# Remove stop words from the list of words\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "# Join the filtered words back into a sentence\n",
    "filtered_text = ' '.join(filtered_words)\n",
    "\n",
    "# Print the original text and the text with stop words removed\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Text with Stop Words Removed:\", filtered_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670a54f",
   "metadata": {},
   "source": [
    "## Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ef8340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text = \"Anant shete is here he is data scientist in world id's\"\n",
    "\n",
    "words = nltk.word_tokenize(text)\n",
    "print(words)\n",
    "\n",
    "# stop_words = stopwords.words('english')\n",
    "\n",
    "# output = [word for word in words if word.lower() not in stop_words]\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb22a517",
   "metadata": {},
   "source": [
    "## Sent Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ffacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text = \"Anant shete is here. he is data scientist in world. He is very calm boy id's\"\n",
    "\n",
    "words = nltk.sent_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c842250",
   "metadata": {},
   "source": [
    "## WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0940362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WhitespaceTokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "# Sample text data\n",
    "text = \" This is a simple example of WhitespaceTokenizer id's.\"\n",
    "\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Initialize the WhitespaceTokenizer\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "# Tokenize the text using WhitespaceTokenizer\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Print the original text and the tokens\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Whitespace_Tokens:\", tokens)\n",
    "print(\"Word_Tokens:\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c633f2a4",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd90d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample words\n",
    "words = [\"running\", \"flies\", \"happily\", \"better\"]\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Stem the words\n",
    "porter_stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "\n",
    "\n",
    "# Initialize the Snowball Stemmer\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Stem the words\n",
    "snowball_stemmed_words = [snowball_stemmer.stem(word) for word in words]\n",
    "\n",
    "# Print the original words and stemmed words\n",
    "print('Porter Stemmer ')\n",
    "print(\"Original Words:\", words)\n",
    "print(\"Stemmed Words:\", porter_stemmed_words)\n",
    "print(\"*\"*100)\n",
    "print('Snowball Stemmer ')\n",
    "print(\"Original Words:\", words)\n",
    "print(\"Stemmed Words:\", snowball_stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31f917d",
   "metadata": {},
   "source": [
    "## Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4d7e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample words\n",
    "# words = [\"running\", \"flies\", \"happily\", \"better\"]\n",
    "words = ['for', 'billing', 'attached', 'playing', 'catched']\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the words\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "# Print the original words and lemmatized words\n",
    "print(\"Original Words:\", words)\n",
    "print(\"Lemmatized Words:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739133d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "words = [\"running\", \"flies\", \"happily\", \"better\",\"ran\"]\n",
    "\n",
    "stemming =  PorterStemmer()\n",
    "stemming_output = [stemming.stem(word) for word in words]\n",
    "lemmitization = WordNetLemmatizer()\n",
    "lemmitization_output = [lemmitization.lemmatize(word) for word in words]\n",
    "\n",
    "print(\"stemming_output\",stemming_output)\n",
    "print(\"lemmitization_output\",lemmitization_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29093a0a",
   "metadata": {},
   "source": [
    "## Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e17ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the NLTK data for POS tagging\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# Sample text\n",
    "text = \"Part-of-speech tagging is an important task in NLP.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "# Print the original text and POS tags\n",
    "print(\"Original Text:\", text)\n",
    "print(\"POS Tags:\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2b62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This block of code snippet is stolen from our blog on Stopwords and Filtering \"\"\"\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "text = \"This is an example text for stopword removal and filtering. This is done using NLTK's stopwords #### $%^$.\"  \n",
    "words = nltk.word_tokenize(text)\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# Extending the stopwords list\n",
    "stopwords.extend(string.punctuation)\n",
    "\n",
    "# Remove stop words and tokens with length < 2\n",
    "cleaned = [word.lower() for word in words if (word not in stopwords) and len(word) > 2]\n",
    "# cleaned = [word for word in words if (word.lower() not in stopwords) and len(word) > 2] stopwords will not be pos_tag use this\n",
    "\"\"\" End of stolen code \"\"\"\n",
    "\n",
    "# Assign POS Tags to the words\n",
    "tagged = nltk.pos_tag(cleaned)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8245095",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda78c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "[('this', 'DT'), ('example', 'NN'), ('text', 'NN'), ('stopword', 'NN'), ('removal', 'NN'), ('filtering', 'VBG'),\n",
    " ('this', 'DT'), ('done', 'VBN'), ('using', 'VBG'), ('nltk', 'JJ'), ('stopwords', 'NNS')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bdba16",
   "metadata": {},
   "source": [
    "## TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a11bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"This is a sample document.\",\n",
    "    \"TF-IDF stands for Term Frequency-Inverse Document Frequency.\",\n",
    "    \"It is a numerical statistic used in information retrieval.\",\n",
    "    \"The TF-IDF score increases with the frequency of a term in a document.\",\n",
    "    \"A high TF-IDF score indicates that a term is important in a document.\",\n",
    "]\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the TF-IDF matrix to a dense array\n",
    "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
    "\n",
    "# Get the feature (term) names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "import pandas as pd\n",
    "tfidf_df = pd.DataFrame(data=tfidf_matrix_dense, columns=feature_names)\n",
    "\n",
    "# Print the TF-IDF matrix\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c0011",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences for training\n",
    "sentences = [\n",
    "    [\"I\", \"love\", \"machine\", \"learning\"],\n",
    "    [\"Word2Vec\", \"is\", \"an\", \"embedding\", \"technique\"],\n",
    "    [\"It\", \"learns\", \"word\", \"representations\"],\n",
    "]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Find the vector representation of a word\n",
    "vector = model.wv['learning']\n",
    "\n",
    "# Find similar words to a given word\n",
    "similar_words = model.wv.most_similar('machine')\n",
    "\n",
    "print(\"Vector for 'learning':\", vector)\n",
    "print(\"Similar words to 'machine':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67aac58",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21fe403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install glove_python\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "# Sample sentences for training (in this case, just a single sentence)\n",
    "sentences = [[\"I\", \"love\", \"machine\", \"learning\"]]\n",
    "\n",
    "# Create a corpus from the training sentences\n",
    "corpus = Corpus()\n",
    "corpus.fit(sentences, window=10)\n",
    "\n",
    "# Create a GloVe model and train it\n",
    "glove = Glove(no_components=100, learning_rate=0.05)\n",
    "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "\n",
    "# Find the vector representation of a word\n",
    "vector = glove.word_vectors[glove.dictionary['learning']]\n",
    "\n",
    "# Find similar words to a given word\n",
    "similar_words = glove.most_similar('machine', number=5)\n",
    "\n",
    "print(\"Vector for 'learning':\", vector)\n",
    "print(\"Similar words to 'machine':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d52b371",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdbc8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)                                                                                                                                                                                                                                                         \n",
    "\n",
    "# Convert the sparse matrix to a dense array\n",
    "X_dense = X.toarray()\n",
    "\n",
    "# Get the feature (word) names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "import pandas as pd\n",
    "bow_df = pd.DataFrame(data=X_dense, columns=feature_names)\n",
    "\n",
    "# Print the Bag of Words representation\n",
    "print(\"Bag of Words (BoW) Matrix:\")\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a76aa",
   "metadata": {},
   "source": [
    "## Text Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ef9b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Sample text\n",
    "text = \"TextBlob is a simple and powerful library for processing textual data.\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Tokenization (breaking text into words or sentences)\n",
    "words = blob.words\n",
    "sentences = blob.sentences\n",
    "\n",
    "# Part-of-speech tagging\n",
    "pos_tags = blob.tags\n",
    "\n",
    "# Sentiment analysis (polarity and subjectivity)\n",
    "polarity = blob.sentiment.polarity\n",
    "subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "# Translation to another language\n",
    "translated_blob = blob.translate(to='fr')  # Translate to French\n",
    "\n",
    "# Spell checking and correction\n",
    "corrected_blob = blob.correct()\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Words:\", words)\n",
    "print(\"Sentences:\", sentences)\n",
    "print(\"Part-of-Speech Tags:\", pos_tags)\n",
    "print(\"Sentiment Polarity:\", polarity)\n",
    "print(\"Sentiment Subjectivity:\", subjectivity)\n",
    "print(\"Translation to French:\", translated_blob)\n",
    "print(\"Corrected Text:\", corrected_blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b55860",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "601c22fd",
   "metadata": {},
   "source": [
    "SpaCy is a popular and efficient library for natural language processing (NLP) tasks in Python. \n",
    "\n",
    "It provides pre-trained models and tools for tasks like tokenization, part-of-speech tagging, named entity recognition, and more. \n",
    "\n",
    "To use spaCy, you'll need to install it and download a language model.\n",
    "\n",
    "Here's a simple example of using spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U pip setuptools wheel\n",
    "# !pip install -U spacy\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load a spaCy language model (e.g., English)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Sample text\n",
    "text = \"SpaCy is a powerful library for NLP tasks. I am Sachin Wandre\"\n",
    "\n",
    "# Process the text using the spaCy pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "# Part-of-speech tagging\n",
    "pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "ner_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Text:\", text)\n",
    "print('*'*100,\"\\n\")\n",
    "print(\"Tokens:\", tokens)\n",
    "print('*'*100,\"\\n\")\n",
    "print(\"Part-of-Speech Tags:\", pos_tags)\n",
    "print('*'*100,\"\\n\")\n",
    "print(\"Named Entity Recognition (NER):\", ner_entities)\n",
    "print('*'*100,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02910daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774e5023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,LogisticRegression,Lasso,Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\n",
    "from sklearn.svm import SVC,SVR\n",
    "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier,AdaBoostRegressor,AdaBoostClassifier,GradientBoostingRegressor,GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "from sklearn.cluster import KMeans,\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,recall_score,precision_score,f1_score,roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer,SnowballStemmer\n",
    "porter = PorterStemmer()\n",
    "porter.stem(word)\n",
    "\n",
    "snow = SnowballStemmer()\n",
    "snow.stem(word)\n",
    "\n",
    "wordnet = WordNetLemmatizer()\n",
    "wordnet.lemmatize(word)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from glove import Corpus,Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fcc96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from glove import Corpus, Glove\n",
    "from sklearn.feature_extraction.text import CountVectorizer (Bag of Words)\n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "words = nltk.word_tokenize(text)\n",
    "words = nltk.sent_tokenize(text)\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "# Initialize the WhitespaceTokenizer\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "# Tokenize the text using WhitespaceTokenizer\n",
    "tokens = tokenizer.tokenize(text)\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "Part of Speech\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# Tokenize the text into words\n",
    "words = nltk.word_tokenize(text)\n",
    "# Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07748250",
   "metadata": {},
   "source": [
    "## Tokenization Using nltk.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer,sent_tokenize,word_tokenize\n",
    "text = \" This is a simple example of WhitespaceTokenizer id's.\"\n",
    "s = sent_tokenize(text)\n",
    "w = word_tokenize(text)\n",
    "wh = WhitespaceTokenizer() \n",
    "whi = wh.tokenize(text)\n",
    "print(\"sent_tokenize\",s)\n",
    "print('word_tokenize',w)\n",
    "print('WhitespaceTokenizer',whi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad4034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = nltk.sent_tokenize(text)\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55132fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decorator function to convert to lowercase\n",
    "def lowercase_decorator(function):\n",
    "   def wrapper():\n",
    "       func = function()\n",
    "       string_lowercase = func.lower()\n",
    "       return string_lowercase\n",
    "   return wrapper\n",
    "# decorator function to split words\n",
    "def splitter_decorator(function):\n",
    "   def wrapper():\n",
    "       func = function()\n",
    "       string_split = func.split()\n",
    "       return string_split\n",
    "   return wrapper\n",
    "@splitter_decorator # this is executed next\n",
    "@lowercase_decorator # this is executed first\n",
    "def hello():\n",
    "   return 'Hello World'\n",
    "hello()   # output => [ 'hello' , 'world' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50a453",
   "metadata": {},
   "source": [
    "## Apply function to remove stopwords from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1596c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK stopwords (run only once)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Assuming your DataFrame has a column named 'text' that contains the text data\n",
    "# Replace 'dataframe' and 'text_column' with your DataFrame and column names\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_list = text.split()\n",
    "    filtered_words = [word for word in word_list if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply the function to the DataFrame column containing text\n",
    "dataframe['text_without_stopwords'] = dataframe['text_column'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b5c17",
   "metadata": {},
   "source": [
    "## Apply function for word_tokenize on dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bb0136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Download NLTK tokenizers (run only once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Assuming your DataFrame has a column named 'text' that contains the text data\n",
    "# Replace 'dataframe' and 'text_column' with your DataFrame and column names\n",
    "\n",
    "# Function to tokenize words\n",
    "def word_tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "# Apply the function to the DataFrame column containing text\n",
    "dataframe['tokenized_words'] = dataframe['text_column'].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c4a75a",
   "metadata": {},
   "source": [
    "## Apply function for sent_tokenize on dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f256ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Assuming your DataFrame has a column named 'text' that contains the text data\n",
    "# Replace 'dataframe' and 'text_column' with your DataFrame and column names\n",
    "\n",
    "# Function to tokenize sentences\n",
    "def sentence_tokenize(text):\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "# Apply the function to the DataFrame column containing text\n",
    "dataframe['tokenized_sentences'] = dataframe['text_column'].apply(sentence_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c0c396",
   "metadata": {},
   "source": [
    "## Apply function for WhiteSpaceTokenizer  on dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60492623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "# Define the WhiteSpaceTokenizer instance\n",
    "tokenizer = WhiteSpaceTokenizer()\n",
    "\n",
    "# Function to apply the tokenizer on a column in the DataFrame\n",
    "def tokenize_text(text):    \n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "# Apply the tokenizer function to the DataFrame\n",
    "df['tokenized_text'] = df['text_column'].apply(tokenize_text)\n",
    "\n",
    "print(df['tokenized_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24559545",
   "metadata": {},
   "source": [
    "## Apply function on dataframe for PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4974db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download punkt tokenizer if not downloaded\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Function to apply the Porter Stemmer on a column in the DataFrame\n",
    "def apply_porter_stemmer(text):\n",
    "    # text = row['text_column']\n",
    "    stemmed_text = ' '.join([porter.stem(word) for word in nltk.word_tokenize(text)])\n",
    "    return stemmed_text\n",
    "\n",
    "# Apply the Porter Stemmer function to the DataFrame\n",
    "df['stemmed_text'] = df['text_column'].apply(apply_porter_stemmer)\n",
    "\n",
    "print(df['stemmed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40abaf7",
   "metadata": {},
   "source": [
    "## Apply function on dataframe for SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e83ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download punkt tokenizer if not downloaded\n",
    "\n",
    "# Initialize the Snowball Stemmer for a specific language (e.g., English)\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "# Function to apply the Snowball Stemmer on a column in the DataFrame\n",
    "def apply_snowball_stemmer(row):\n",
    "    text = row['text_column']\n",
    "    stemmed_text = ' '.join([snowball.stem(word) for word in nltk.word_tokenize(text)])\n",
    "    return stemmed_text\n",
    "\n",
    "# Apply the Snowball Stemmer function to the DataFrame\n",
    "df['stemmed_text'] = df.apply(apply_snowball_stemmer, axis=1)\n",
    "\n",
    "print(df['stemmed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab860f3",
   "metadata": {},
   "source": [
    "## Apply function on dataframe for WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa399199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download punkt tokenizer if not downloaded\n",
    "nltk.download('wordnet')  # Download WordNet if not downloaded\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to apply the WordNet Lemmatizer on a column in the DataFrame\n",
    "def apply_wordnet_lemmatizer(row):\n",
    "    text = row['text_column']\n",
    "    lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text)])\n",
    "    return lemmatized_text\n",
    "\n",
    "# Apply the WordNet Lemmatizer function to the DataFrame\n",
    "df['lemmatized_text'] = df.apply(apply_wordnet_lemmatizer, axis=1)\n",
    "\n",
    "print(df['lemmatized_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1c5e27",
   "metadata": {},
   "source": [
    "## Apply TfidfVectorizer on dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd412d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'text_column': [\n",
    "        'This is a sentence.',\n",
    "        'Another example here.',\n",
    "        'Text to vectorize for TF-IDF.'\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data in the DataFrame\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text_column'])\n",
    "\n",
    "# Convert the TF-IDF matrix to an array for easier visualization (optional)\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Create a DataFrame to display the TF-IDF values (optional)\n",
    "tfidf_df = pd.DataFrame(tfidf_array, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc909d89",
   "metadata": {},
   "source": [
    "## Apply Word2Vec on dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aeb7c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sachin\n",
      "[nltk_data]     Wandre\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.13233014e-03 -4.45734849e-03 -1.06835144e-03  1.00642710e-03\n",
      " -1.91050975e-04  1.14818686e-03  6.11383095e-03 -2.02962838e-05\n",
      " -3.24592204e-03 -1.51068275e-03  5.89733245e-03  1.51411747e-03\n",
      " -7.24213547e-04  9.33331158e-03 -4.92126541e-03 -8.38353764e-04\n",
      "  9.17545147e-03  6.74946513e-03  1.50287175e-03 -8.88254959e-03\n",
      "  1.14880584e-03 -2.28825072e-03  9.36818309e-03  1.20987650e-03\n",
      "  1.49011170e-03  2.40643439e-03 -1.83602748e-03 -4.99957008e-03\n",
      "  2.32419639e-04 -2.01422395e-03  6.60089497e-03  8.94018169e-03\n",
      " -6.74724230e-04  2.97706923e-03 -6.10765489e-03  1.69926288e-03\n",
      " -6.92627253e-03 -8.69406015e-03 -5.90023678e-03 -8.95645376e-03\n",
      "  7.27753108e-03 -5.77198202e-03  8.27640016e-03 -7.24353036e-03\n",
      "  3.42168007e-03  9.67502035e-03 -7.78544368e-03 -9.94510762e-03\n",
      " -4.32914170e-03 -2.68311962e-03 -2.71307683e-04 -8.83149542e-03\n",
      " -8.61760974e-03  2.80015357e-03 -8.20634235e-03 -9.06935334e-03\n",
      " -2.34053191e-03 -8.63175653e-03 -7.05660321e-03 -8.40109959e-03\n",
      " -3.01384978e-04 -4.56429366e-03  6.62711356e-03  1.52721489e-03\n",
      " -3.34150065e-03  6.10895408e-03 -6.01329003e-03 -4.65618493e-03\n",
      " -7.20745930e-03 -4.33651777e-03 -1.80934567e-03  6.48960192e-03\n",
      " -2.77044624e-03  4.91892919e-03  6.90444745e-03 -7.46376114e-03\n",
      "  4.56491252e-03  6.12695562e-03 -2.95447651e-03  6.62499620e-03\n",
      "  6.12583011e-03 -6.44344045e-03 -6.76458888e-03  2.53902236e-03\n",
      " -1.62381353e-03 -6.06506970e-03  9.49924346e-03 -5.13020856e-03\n",
      " -6.55408436e-03 -1.19904675e-04 -2.70148856e-03  4.44340578e-04\n",
      " -3.53740319e-03 -4.19310498e-04 -7.08681822e-04  8.22786533e-04\n",
      "  8.19482747e-03 -5.73664904e-03 -1.65954779e-03  5.57165220e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download punkt tokenizer if not downloaded\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'text_column': [\n",
    "        'This is a sentence.',\n",
    "        'Another example here.',\n",
    "        'Text to apply Word2Vec.'\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Tokenize the text in the DataFrame\n",
    "tokenized_text = [nltk.word_tokenize(text) for text in df['text_column']]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Example: Get Word Vectors for a word\n",
    "word_vectors = word2vec_model.wv\n",
    "vector_for_word = word_vectors['example']  # Example for the word 'example'\n",
    "\n",
    "print(vector_for_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c0185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
